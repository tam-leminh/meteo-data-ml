{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import pyowm\n",
    "import time\n",
    "import math\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, RationalQuadratic, ConstantKernel as C\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "import CustomKernels\n",
    "from CustomDistances import sq_distance, hv_distance\n",
    "from matplotlib.patches import Path, PathPatch\n",
    "from pyearth import Earth\n",
    "\n",
    "\n",
    "##Formatting functions\n",
    "def format_for_learning(xlat, xlon, temp):\n",
    "    X = np.column_stack((xlat, xlon))\n",
    "    Y = np.asarray(temp).reshape(len(temp),1)\n",
    "    return X, Y\n",
    "\n",
    "def format_grid_for_prediction(ylat, ylon):\n",
    "    grid = np.column_stack((np.hstack((ylat)),np.hstack((ylon))))\n",
    "    return grid\n",
    "    \n",
    "def format_prediction_to_grid(prediction, nrow, ncol):\n",
    "    matrix = np.reshape(prediction, (nrow,ncol))  \n",
    "    return matrix\n",
    "    \n",
    "    \n",
    "##Nearest neighbor with plane euclidian distance\n",
    "def NNI_temp(X, Y, grid):\n",
    "    \n",
    "    nsample = X.shape[0]\n",
    "    npreds = grid.shape[0]\n",
    "    prediction = np.empty(npreds)\n",
    "    \n",
    "    ##Vectorized operation is slower than for-loop\n",
    "    #distance = hv_distance(grid[:,0,None], grid[:,1,None], X[:,0], X[:,1])\n",
    "    #print(distance)\n",
    "    #prediction = y[np.argmin(distance, axis=1)]\n",
    "    \n",
    "    for k in range(0, npreds):\n",
    "        distance = hv_distance(grid[k][0], grid[k][1], X[:,0], X[:,1])\n",
    "        prediction[k] = Y[np.argmin(distance)]\n",
    "            \n",
    "    return prediction\n",
    "            \n",
    "    \n",
    "##Inverse distance weighting\n",
    "##Param: radius of the IDW, if no points in the radius -> NNI \n",
    "def IDW_temp(X, Y, grid, radius):\n",
    "    \n",
    "    nsample = X.shape[0]\n",
    "    npreds = grid.shape[0]\n",
    "    prediction = np.empty(npreds)\n",
    "    for k in range(0, npreds):\n",
    "        numerator2 = 0\n",
    "        denominator2 = 0\n",
    "        distance = hv_distance(grid[k][0], grid[k][1], X[:,0], X[:,1])\n",
    "        idx_in_radius = np.where(distance < radius)\n",
    "        if idx_in_radius[0].size == 0:\n",
    "            prediction[k] = Y[np.argmin(distance)]\n",
    "        else:\n",
    "            numerator = np.sum(Y[idx_in_radius]/distance[idx_in_radius])\n",
    "            denominator = np.sum(1/distance[idx_in_radius])\n",
    "            prediction[k] = numerator/denominator\n",
    "            \n",
    "    return prediction\n",
    "        \n",
    "        \n",
    "def gaussprocess_temp(X, Y, grid):\n",
    "    \n",
    "    kernel = C(1.0)*RBF(length_scale=[10.0, 10.0]) + WhiteKernel(0.1)\n",
    "    #gpr = GaussianProcessRegressor(kernel=kernel, alpha = 0, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=10, normalize_y=True, random_state=0).fit(X, y)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, alpha = 0, optimizer=None, n_restarts_optimizer=10, normalize_y=True, random_state=0).fit(X, y)\n",
    "    print(\"score: %f\" % gpr.score(X, Y))\n",
    "    preds = gpr.predict(grid)\n",
    "    print(gpr.kernel_)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def GWR_temp(X, Y, grid):\n",
    "    \n",
    "    kernel = C(1.0)*CustomKernels.RBF(length_scale_bounds=[10.0,1000.0], metric='haversine') + WhiteKernel(0.1)\n",
    "    gwr = GaussianProcessRegressor(kernel=kernel, alpha = 0, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=10, normalize_y=True, random_state=0).fit(X, y)\n",
    "    #gwr = GaussianProcessRegressor(kernel=kernel, alpha = 0, optimizer=None, n_restarts_optimizer=0, normalize_y=True, random_state=0).fit(X, y)\n",
    "    print(\"score: %f\" % gwr.score(X, Y))\n",
    "    preds = gwr.predict(grid)\n",
    "    print(gwr.kernel_)\n",
    "    \n",
    "    return preds\n",
    "    \n",
    "\n",
    "def regressiontree_temp(X, Y, grid, max_depth):\n",
    "    \n",
    "    rtree = DecisionTreeRegressor(max_depth=max_depth).fit(X, Y)\n",
    "    preds = rtree.predict(grid)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def randomforest_temp(X, Y, grid, max_ntree, max_depth, random_state):\n",
    "    \n",
    "    rforest = RandomForestRegressor(n_estimators=max_ntree, max_features='auto', \n",
    "                                    max_depth=max_depth, random_state=random_state).fit(X, Y)\n",
    "    preds = rforest.predict(grid)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def extratrees_temp(X, Y, grid, max_ntree, max_depth, random_state):\n",
    "    \n",
    "    extrees = ExtraTreesRegressor(n_estimators=max_ntree, max_features='auto', \n",
    "                                    max_depth=max_depth, random_state=random_state).fit(X, Y)\n",
    "    preds = extrees.predict(grid)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def SVR_temp(X, Y, grid):\n",
    "    \n",
    "    svr = SVR(gamma='auto', C=10.0, epsilon=5.0).fit(X, Y)\n",
    "    preds = svr.predict(grid)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "def regressionsplines_temp(X, Y, grid, max_degree, penalty):\n",
    "    \n",
    "    splin = Earth(max_degree = max_degree, penalty = penalty).fit(X, Y)\n",
    "    preds = splin.predict(grid)\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read cities list\n",
    "obs = pd.read_csv(\"data/current-version/Temp-2019_01_04-15_47.csv\", ',')\n",
    "obs = obs.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "##Get cities coordinates and ID\n",
    "nam = obs['City'].tolist()\n",
    "lon = obs['Lon'].tolist()\n",
    "lat = obs['Lat'].tolist()\n",
    "ids = obs['ID'].tolist()\n",
    "temps = obs['Temp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Map boundaries\n",
    "lon_min = -15.56\n",
    "lat_min = 24.65\n",
    "lon_max = 49.88\n",
    "lat_max = 79.17\n",
    "\n",
    "##Create map\n",
    "map = Basemap(llcrnrlon=lon_min,llcrnrlat=lat_min,urcrnrlon=lon_max,urcrnrlat=lat_max, resolution = 'l', epsg=4668)\n",
    "\n",
    "##Interpolation resolution\n",
    "nx = 100\n",
    "ny = 100\n",
    "\n",
    "glons, glats = map.makegrid(nx, ny)\n",
    "gx, gy = map(glons, glats)\n",
    "\n",
    "##Format data for interpolation\n",
    "X_learn, y_learn = format_for_learning(lat, lon, temps)\n",
    "grid = format_grid_for_prediction(glats, glons)\n",
    "\n",
    "##Partition data between train and test sets\n",
    "train_size = X_learn.shape[0]*4/5\n",
    "indices = np.random.permutation(X_learn.shape[0])\n",
    "training_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "X_training, X_test = X_learn[training_idx,:], X_learn[test_idx,:]\n",
    "y_training, y_test = y_learn[training_idx,:], y_learn[test_idx,:]\n",
    "\n",
    "tic = time.time()\n",
    "##Choose the interpolation model\n",
    "preds = NNI_temp(X_learn, y_learn, grid)\n",
    "#preds = IDW_temp(X_learn, y_learn, grid, 500)\n",
    "#preds = gaussprocess_temp(X_learn, y_learn, grid)\n",
    "#preds = GWR_temp(X_learn, y_learn, grid)\n",
    "#preds = regressiontree_temp(X_learn, y_learn, grid, 10)\n",
    "#preds = randomforest_temp(X_learn, y_learn, grid, max_ntree=1000, max_depth=10, random_state=69)\n",
    "#preds = extratrees_temp(X_learn, y_learn, grid, max_ntree=1000, max_depth=10, random_state=69)\n",
    "#preds = SVR_temp(X_learn, y_learn, grid)\n",
    "#preds = regressionsplines_temp(X_learn, y_learn, grid, 3, 1.0)\n",
    "toc = time.time()\n",
    "print(\"Time: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "\n",
    "##Format the predictions for plotting\n",
    "predict = format_prediction_to_grid(preds, nx, ny)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "\n",
    "map.drawmapboundary(fill_color='white')\n",
    "#map.fillcontinents(color='coral',lake_color='white')\n",
    "map.drawcoastlines()\n",
    "\n",
    "x, y = map(lon, lat)\n",
    "cities_out_bounds = []\n",
    "\n",
    "##Create annotations for temperature and only keep cities in bound\n",
    "for i in range(0,len(x)):\n",
    "    if lon[i] > lon_min and lon[i] < lon_max and lat[i] > lat_min and lat[i] < lat_max:\n",
    "        plt.text(x[i], y[i], \"{0:.1f}\".format(temps[i]),fontsize=10,fontweight='bold', ha='center',va='center',color='k')\n",
    "    else:\n",
    "        cities_out_bounds.append(nam[i])\n",
    "\n",
    "##Plot contours\n",
    "clevs = [-24,-22,-20,-18,-16,-14,-12,-10,-8,-6,-4,-2,0,2,4,6,8,10,12,14,16,18,20,22]\n",
    "cs = map.contourf(gx,gy,predict,clevs,cmap='Spectral_r')\n",
    "\n",
    "##Display colorbar\n",
    "cbar = map.colorbar(cs,location='bottom',pad=\"5%\")\n",
    "cbar.set_label('degrees Celsius')\n",
    "\n",
    "##Getting the limits of the map:\n",
    "x0,x1 = ax.get_xlim()\n",
    "y0,y1 = ax.get_ylim()\n",
    "map_edges = np.array([[x0,y0],[x1,y0],[x1,y1],[x0,y1]])\n",
    "\n",
    "##Getting all polygons used to draw the coastlines of the map\n",
    "polys = [p.boundary for p in map.landpolygons]\n",
    "\n",
    "##Combining with map edges\n",
    "polys = [map_edges]+polys[:]\n",
    "\n",
    "##Creating a PathPatch\n",
    "codes = [\n",
    "    [Path.MOVETO] + [Path.LINETO for p in p[1:]]\n",
    "    for p in polys\n",
    "]\n",
    "polys_lin = [v for p in polys for v in p]\n",
    "codes_lin = [c for cs in codes for c in cs]\n",
    "path = Path(polys_lin, codes_lin)\n",
    "patch = PathPatch(path,facecolor='cyan',lw=0)\n",
    "\n",
    "##Masking the data:\n",
    "ax.add_patch(patch)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "toc = time.time()\n",
    "print(\"Time: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_learn.shape[0]*4/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
